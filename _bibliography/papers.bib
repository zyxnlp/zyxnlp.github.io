---
---

@string{aps = {American Physical Society,}}

@article{Zhou2021ToBC,
  title={To be Closer: Learning to Link up Aspects with Opinions},
  author={Yuxiang Zhou and Lejian Liao and Yang Gao and Zhanming Jie and Wei Lu},
  journal={In Proceedings of EMNLP},
  year={2021},
  abstract={Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA). However, the trees obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between aspects and corresponding opinion words by learning an aspect-centric tree structure. The aspect and opinion words are expected to be closer along such tree structure compared to the standard dependency parse tree. The learning process allows the tree structure to adaptively correlate the aspect and opinion words, enabling us to better identify the polarity in the ABSA task. We conduct experiments on five aspect-based sentiment datasets, and the proposed model significantly outperforms recent strong baselines. Furthermore, our thorough analysis demonstrates the average distance between aspect and opinion words are shortened by at least 19% on the standard SemEval Restaurant14 dataset.},
  html={https://arxiv.org/abs/2109.08382},
  volume={abs/2109.08382},
  selected={true}
}

@article{Zhou2021TopicBERTAT,
title={TopicBERT: A Topic-Enhanced Neural Language Model Fine-Tuned for Sentiment Classification.},
author={Yuxiang Zhou and Lejian Liao and Yang Gao and Rui Wang and Heyan Huang},
journal={IEEE transactions on neural networks and learning systems},
year={2021},
abstract={Sentiment classification is a form of data analytics where people's feelings and attitudes toward a topic are mined from data. This tantalizing power to ``predict the zeitgeist'' means that sentiment classification has long attracted interest, but with mixed results. However, the recent development of the BERT framework and its pretrained neural language models is seeing new-found success for sentiment classification. BERT models are trained to capture word-level information via mask language modeling and sentence-level contexts via next sentence prediction tasks. Out of the box, they are adequate models for some natural language processing tasks. However, most models are further fine-tuned with domain-specific information to increase accuracy and usefulness. Motivated by the idea that a further fine-tuning step would improve the performance for downstream sentiment classification tasks, we developed TopicBERT--a BERT model fine-tuned to recognize topics at the corpus level in addition to the word and sentence levels. TopicBERT comprises two variants: TopicBERT-ATP (aspect topic prediction), which captures topic information via an auxiliary training task, and TopicBERT-TA, where topic representation is directly injected into a topic augmentation layer for sentiment classification. With TopicBERT-ATP, the topics are predetermined by an LDA mechanism and collapsed Gibbs sampling. With TopicBERT-TA, the topics can change dynamically during the training. Experimental results show that both approaches deliver the state-of-the-art performance in two different domains with SemEval 2014 Task 4. However, in a test of methods, direct augmentation outperforms further training. Comprehensive analyses in the form of ablation, parameter, and complexity studies accompany the results.},
html={https://ieeexplore.ieee.org/document/9508773},
selected={true}
}

@article{Zhou2021ExtractingSF,
title={Extracting salient features from convolutional discriminative filters},
author={Yuxiang Zhou and Lejian Liao and Yang Gao and Heyan Huang},
journal={Information Sciences},
year={2021},
volume={558},
pages={265-279},
abstract={Convolutional neural networks (CNN) have been widely used in various tasks, largely dueto their ability to efficiently extract n-gram features for text analysis and document repre-sentation. In this paper, we intend to insight the CNN model regarding its capability on textanalysis. Vanilla CNNs do have weaknesses when it comes to the representation and fea-ture extraction. Duplicate filters are inevitable with vanilla CNNs, which reduces the dis-criminative power of the representations. In addition, the current pooling operationseither limit the CNN to the local optimum (i.e., max pooling) or they do not consider theimportance of all features (i.e., mean pooling). In this paper, we propose two modulesfor vanilla CNNs to overcome these shortcomings. The first equips the CNN with discrim-inative filters (distinct filters with maximised divergence) and the second provides theability to comprehensively extract all salient features. Specifically, our model increasesthe discriminative power of the model by maximizing the distance between different fil-ters, and a novel global pooling mechanism for feature extraction. Validation tests againststate-of-the-art baselines on five benchmark classification datasets achieve the competi-tive performance of our proposed model. Furthermore, visualization on upgrade filtersand pooling features verify our hypothesis that the proposed model can receive discrimi-native filters and salient features.},
html={https://www.sciencedirect.com/science/article/pii/S0020025521000165?via%3Dihub},
selected={true}
}

@article{Zhou2020ADC,
title={A Discriminative Convolutional Neural Network with Context-aware Attention},
author={Yuxiang Zhou and Lejian Liao and Yang Gao and Heyan Huang and Xiaochi Wei},
journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
year={2020},
volume={11},
pages={1 - 21},
html={https://dl.acm.org/doi/10.1145/3397464}
}

@article{Wenbo2019ConceptPN,
title={Concept Pointer Network for Abstractive Summarization},
author={Wang Wenbo and Yang Gao and Huang Heyan and Yuxiang Zhou},
journal={In Proceedings of EMNLP},
year={2019},
volume={abs/1910.08486},
abstract={A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details. Inspired by the popular pointer generator sequence-to-sequence model, this paper presents a concept pointer network for improving these aspects of abstractive summarization. The network leverages knowledge-based, context-aware conceptualizations to derive an extended set of candidate concepts. The model then points to the most appropriate choice using both the concept set and original source text. This joint approach generates abstractive summaries with higher-level semantic concepts. The training model is also optimized in a way that adapts to different data, which is based on a novel method of distant-supervised learning guided by reference summaries and testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC-2004 and Gigaword datasets. A human evaluation of the modelâ€™s abstractive abilities also supports the quality of the summaries produced within this framework.},
html={https://aclanthology.org/D19-1304/}
}

@article{Qiao2019ASW,
title={A simple water cycle algorithm with percolation operator for clustering analysis},
author={Shilei Qiao and Yongquan Zhou and Yuxiang Zhou and Rui Wang},
journal={Soft Computing},
year={2019},
volume={23},
pages={4081-4095},
html={https://link.springer.com/article/10.1007%2Fs00500-018-3057-5}
}

@article{Zhou2017ASM,
title={A simplex method-based social spider optimization algorithm for clustering analysis},
author={Yongquan Zhou and Yuxiang Zhou and Qifang Luo and Mohamed Abdel-Basset},
journal={Engineering Applications of Artificial Intelligence (EAAI)},
year={2017},
volume={64},
pages={67-82},
html={https://www.sciencedirect.com/science/article/abs/pii/S0952197617301239?via%3Dihub}
}

@article{Zhou2016ANC,
title={A Novel Complex-Valued Social Spider Optimization Algorithm},
author={Yuxiang Zhou and Yongquan Zhou and Qifang Luo and Shilei Qiao and Rui Wang},
journal={Journal of Computational and Theoretical Nanoscience},
year={2016},
volume={13},
pages={3273-3289},
html={https://www.ingentaconnect.com/content/asp/jctn/2016/00000013/00000005/art00080}
}

@inproceedings{Zhou2015DriftOF,
title={Drift Operator for States of Matter Search Algorithm},
author={Yuxiang Zhou and Yongquan Zhou and Qifang Luo and Shilei Qiao and Rui Wang},
booktitle={International Conference on Intelligent Computing (ICIC)},
year={2015},
html={https://link.springer.com/chapter/10.1007%2F978-3-319-22053-6_7}
}

@inproceedings{Qiao2015SelfadaptivePB,
title={Self-adaptive Percolation Behavior Water Cycle Algorithm},
author={Shilei Qiao and Yongquan Zhou and Rui Wang and Yuxiang Zhou},
booktitle={International Conference on Intelligent Computing (ICIC)},
year={2015},
html={https://link.springer.com/chapter/10.1007%2F978-3-319-22180-9_9}
}




